# -*- coding: utf-8 -*-
"""DMP - Pollution Predictor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vplOK1UgY0yTNDJMyOkRVo92CtFIBKcD

# ***Preface***

This Collab was made in accordance to Data Mining Project for CSE PDEU Sem 5.

Team: DMP_27 

---

19BCP016 - Bhagvatsinh Jadeja / 
19BCP093 - Pathik Viramgama / 
19BCP137 - Vatsal Sevalia

---

# ***Importing and Downloading Major Libraries***
"""

!pip install --upgrade scikit-learn

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""# **Working** **on** **The** ***Data***"""

#Loading data into variable using Pandas
data_df = pd.read_csv('/content/PM2.5 Global Air Pollution 2010-2017.csv')

#First Look at Dataset
data_df

#Target Attribute
data_df['2017']

#Features
list(data_df.columns)

#checking for null values in whole dataset
data_df.isnull()

#there is no null values found in this dataset
#uncomment respectively if you add an extra to check the validity of project

#data_df.fillna(method ='pad') 
#data_df.fillna(method ='bfill')
#data_df.interpolate(method ='linear', limit_direction ='forward')
#data_df.dropna(axis = 0, how ='any')

#data_df.isnull()

#Checking for null values/ confirming the previous table using for loop for each column
for i in range(2010,2017):
  i = str(i)
  y = data_df[i].isnull().values.any()
  print(y)

#heatmap of data proving there is no null values
sns.heatmap(data_df.isnull(),yticklabels=False, annot=True)

#Outlier detection
temp = data_df.drop(['Country Name','Country Code'],axis = 1)

#Our data's main label is country rather than the year. So we need to detect outlier row wise instead of column wise
for ind,row in temp.head().iterrows():
  sns.boxplot(row.tolist())

#NOTE: If you get the futureWarning error it is because there is no label given. However as we can see in the graph there is no outlier

"""# ***Splitting The Data***"""

#Preparing the data to get splitted
x = data_df.drop(['2017'], axis = 1)
y = data_df['2017']

#splitting into train and test
#best output was obtained for 30-70 split with no randomization 
from sklearn.model_selection import train_test_split
x_train_temp, x_test_temp, y_train_temp, y_test_temp = train_test_split(x,y,test_size = 0.3, random_state = 0)

#saving country names in different variables
test_countries = x_test_temp['Country Name']
train_countries = x_train_temp['Country Name']
test_codes = x_test_temp['Country Code']
train_codes = x_test_temp['Country Code']
name = data_df['Country Name']
code = data_df['Country Code']

#Preparing the Splitted data to load in Model
x_train = x_train_temp.drop(['Country Name','Country Code'], axis = 1)
y_train = y_train_temp
x_test = x_test_temp.drop(['Country Name','Country Code'], axis = 1)
y_test = y_test_temp

"""# ***Generalized Linear Regression Using Gamma Regression***"""

#importing specific library for gama regressor
from sklearn.linear_model import GammaRegressor

#Running the model on our splitted data
modelGamma = GammaRegressor()
modelGamma.fit(x_train, y_train)

#Storing the prediction in variable
y_predGamma = modelGamma.predict(x_test)
y_predGamma

#Calculating Accuracy and RMS Error
from sklearn.metrics import r2_score,mean_squared_error
print(r2_score(y_test,y_predGamma))
print(np.sqrt(mean_squared_error(y_test, y_predGamma)))

#plotting Scatter graph for Actual vs Predicted
plt.figure(figsize=(8,8))
plt.scatter(y_test,y_predGamma, color = 'red')
plt.xlabel('Actual 2017')
plt.ylabel('Predicted 2017')
plt.title('Gamma Regressor Actual 2017 vs. Predicted 2017')

#plotting continous graph for Actual vs Predicted
plt.plot(y_test,y_predGamma ,'g-')

#This model uses Quadratic hypothesis function as evident by graph
#Hence the accuracy is 0.82
#Lets try with a Linear hypothsis function
#best way to address this problem is to use Simple Linear Regression

"""# ***Linear Regression Model***"""

#importing LinearRegression Algorithm
from sklearn.linear_model import LinearRegression

#Running the model on our splitted data
modelLR = LinearRegression()
modelLR.fit(x_train,y_train)

#Storing the prediction in variable
y_predLr = modelLR.predict(x_test)
y_predLr

#plotting Scatter graph for Actual vs Predicted
plt.figure(figsize=(8,8))
plt.scatter(y_test,y_predLr, color = 'red')
plt.xlabel('Actual 2017')
plt.ylabel('Predicted 2017')
plt.title('Actual 2017 vs. Predicted 2017')

#plotting continous graph for Actual vs Predicted
plt.plot(y_test,y_predLr ,'g-')

#Now as we can see in graph we have obtained best results!
#So our accuracy will be better than Gamma Regressor

#Tabular form of Prediction vs Actual and difference in their values
pred_y_df_Lr = pd.DataFrame({'Country Name':test_countries,'Country Code':test_codes, 'Actual Value':y_test,'predicted value':y_predLr, 'Difference': y_test-y_predLr})

#Saving the comparision table in drive
#You can donwload it from Home Page of Files
pred_y_df_Lr.to_csv('/content/Comparision Table.csv')
pred_y_df_Lr

#Calculating Accuracy and RMS Error
from sklearn.metrics import r2_score,mean_squared_error
print("Accuracy: ",r2_score(y_test,y_predLr))
print("Mean Sqaure Root Error: ",np.sqrt(mean_squared_error(y_test, y_predLr)))

"""# **Conclusion**

As the data set was not highly dimensional, the results of simple linear regression is better than advancede linear regressions. Also the data set was squeaky clean and no null values or outliers or redundant values, rows, columns were found. Hence the prediction was 99 percent accurate.

Since all the extensions are useless here we will have final answer from Linear Regression. Uncommnting the data reduction code, changing the directory for your new dataset and dropping respective columns, this code will work for all other datasets as well.
"""